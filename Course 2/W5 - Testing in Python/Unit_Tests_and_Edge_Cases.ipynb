{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Notebook - Unit Tests and Edge Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have some code that makes a list of specific letters found in any string. If you run it, you can see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "my_txt = \"An investment in knowledge pays the best interest.\"\n",
    "\n",
    "def LetterCompiler(txt):\n",
    "    assert(type(txt) != str , \"non string input\")\n",
    "    result = re.findall(r'([a-c]).', txt)\n",
    "    return result\n",
    "\n",
    "print(LetterCompiler(my_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, you can see that the `LetterCompiler( )` function finds all matches for the letters a through c in an input string if followed by another character and returns them as a list of strings, with each string representing one match. Nice.\n",
    "<br><br>\n",
    "But can we be sure that this function will always do what we expect it to do? We need to write code to help us catch mistakes, errors and bugs.  This code should automate the process of checking if the returned value of our code matches the expectations by dynamically feeding into it test cases.  Since we're dynamically feeding in different strings, it would be prudent to create unit tests for our code. We can use the module **unittest** for this. \n",
    "<br><br>\n",
    "Fill in the blanks below to create an automatic unint test that verifies whether input strings have the correct list of string matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestCompiler(unittest.TestCase):\n",
    "\n",
    "    def test_basic(self):\n",
    "        testcase = \"The best preparation for tomorrow is doing your best today.\"\n",
    "        expected = ['b', 'a', 'a', 'b', 'a']\n",
    "        self.assertEqual(LetterCompiler(testcase), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your automatic test is coded, you need to call the `unittest.main( )` function to run the test.  It is important to note that the configuration for running unit tests in Jupyter is different than running unit tests from the command line. Running `unittest.main( )` in Jupyter **will result in an error**.  You can see this by runnig the following cell to execute your automatic test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: /home/jovyan/ (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute '/home/jovyan/'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
     ]
    }
   ],
   "source": [
    "unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! **<font color=red>SystemExit:</font> True** means an error occurred, as expected.  The reason is that `unittest.main( )` looks at *sys.argv*.  In Jupyter, by default, the first parameter of *sys.argv* is what started the Jupyter kernel which is not the case when executing it from the command line.  This default parameter is passed into `unittest.main( )` as an attribute when you don't explicitly pass it attributes and is therefore what causes the error about the kernel connection file not being a valid attribute. Passing an explicit list to `unittest.main( )` prevents it from looking at *sys.argv*. \n",
    "<br><br>Let's pass it the list ['first-arg-is-ignored'] for example.  In addition, we will pass it the parameter *exit = False* to prevent `unittest.main( )` from shutting down the kernel process.  Run the following cell with the *argv* and *exit* parameters passed into `unittest.main( )` to rerun your automatic test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".E\n",
      "======================================================================\n",
      "ERROR: test_two (__main__.TestCompiler2)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-f0e1df8c083b>\", line 11, in test_two\n",
      "    self.assertEqual(LetterCompiler(testcase), expected)\n",
      "  File \"<ipython-input-8-822df5dc89f8>\", line 7, in LetterCompiler\n",
      "    result = re.findall(r'([a-c]).', txt)\n",
      "  File \"/opt/conda/lib/python3.6/re.py\", line 222, in findall\n",
      "    return _compile(pattern, flags).findall(string)\n",
      "TypeError: expected string or bytes-like object\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.042s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fac19f9b6a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv = ['first-arg-is-ignored'], exit = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did your automatic test pass? Was **OK** the result? If not, go back to your automatic test code and make sure you filled in the blanks correctly.  If your automatic test passed, great! You have successfully filled in the gaps to create an automatic test that verifies whether input strings have the correct list of string matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great work so far, but your automatic test includes only one test case.  You need to make it grow.  You can feed in more strings as test cases to test whether your code works in the general case.  But you should also see what happens when you give it some input that you might not expect it to run into under normal operations. \n",
    "<br><br>\n",
    "Edge cases are inputs to code that produce unexpected results, and are found at the extreme ends of the ranges of input we imagine programs will typically work with.  Can you use the cell below to write some edge cases? We've already filled in another test case for you! As it is, this test will run fine. Can you come up with at least one test case that you think could result in a wrong return value? No wrong answers! Feel free to play around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fac19f96e80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestCompiler2(unittest.TestCase):\n",
    "    \n",
    "    def test_two(self):\n",
    "        testcase = \"A b c d e f g h i j k l m n o q r s t u v w x y z\"\n",
    "        expected = ['b', 'c']\n",
    "        self.assertEqual(LetterCompiler(testcase), expected)\n",
    "    \n",
    "    def test_array_input(self):\n",
    "        testcase = [\"abcdedf\"]\n",
    "        self.assertRaises(TypeError, LetterCompiler, testcase)\n",
    "        \n",
    "    def test_num_input(self):\n",
    "        testcase = -2\n",
    "        self.assertRaises(TypeError, LetterCompiler, testcase)\n",
    "\n",
    "\n",
    "unittest.main(argv = ['first-arg-is-ignored'], exit = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you find any edge cases?  If not, continue working on it.  Choosing test cases can be an exercise in creativity.  Coming up with different ways a code might break can be super fun! When you have found an edge case, think about special handling in your script in order for your code to continue to behave correctly.\n",
    "<br>\n",
    "If you are out of ideas: Try removing the spaces and figure out why they were in the example testcase. Does that give you an idea for other tests?\n",
    "<br>\n",
    "When you have found at least one edge case, you are all done with this notebook.  You should take a moment to reflect on what you've done so far.  It's super impressive and it's going to fit nicely in your IT toolkit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
